This is a recurrent neural network architecture inspired by Long Short Term Memory, but with a much simpler architecture.

I will be adding more documentation to describe this architecture shortly. For now, a brief summary is that it has only two gates - one for representing the "current" input, and another that is similar to the Forget Gate of LSTM. But rather than acting to reset the state of the cell, it performs a weighted average, controlling the amount by which the cell is updated to the "current" value, versus retaining its previous state.

So far on a few tests I've done, it appears to perform fairly well, although it tends to need more cell blocks than LSTM to do the same thing.
